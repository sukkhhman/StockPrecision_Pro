{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "!pip install stocksymbol\n",
        "from stocksymbol import StockSymbol\n",
        "\n",
        "api_key = '74e6f192-55d1-4115-a88b-7e8c17e29b9c'\n",
        "ss = StockSymbol(api_key)\n",
        "\n",
        "symbol_list_us = ss.get_symbol_list(market=\"IN\")  # \"us\" or \"america\" will also work\n",
        "\n",
        "file_path = \"/content/ticker\"\n",
        "\n",
        "with open(file_path, \"w\", newline=\"\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Symbol\"])\n",
        "    writer.writerows(zip(symbol_list_us))\n",
        "\n",
        "print(\"List of symbols (IN market) saved to symbol_list.csv file.\")"
      ],
      "metadata": {
        "id": "SDcOafIv4aoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = 'ticker'\n",
        "output_file = 'ticker_cleaned'\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Extract the 'symbol' field and remove other details\n",
        "df['symbol'] = df['Symbol'].apply(lambda x: eval(x)['symbol'])\n",
        "\n",
        "# Drop the original 'Symbol' column\n",
        "df.drop(columns=['Symbol'], inplace=True)\n",
        "\n",
        "# Write the modified DataFrame back to a new CSV file\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(\"Symbols extracted and saved to\", output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odFUYqB85QCL",
        "outputId": "af873370-82b3-4001-a5da-34ddeb6e3347"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbols extracted and saved to ticker_cleaned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = '/content/ticker_cleaned'\n",
        "output_file = 'symbol_info.csv'\n",
        "\n",
        "# Read symbols from the CSV file into a DataFrame\n",
        "symbols_df = pd.read_csv(input_file)\n",
        "\n",
        "# Define the list of desired features\n",
        "desired_features = ['sector','shortName','longName','industry']\n",
        "\n",
        "# Function to fetch information for a symbol\n",
        "def fetch_info(symbol):\n",
        "    try:\n",
        "        # Use the entered symbol with yf.Ticker() to get the information\n",
        "        ticker = yf.Ticker(symbol)\n",
        "        info = ticker.info\n",
        "\n",
        "        # Create a dictionary to store selected features for the symbol\n",
        "        selected_info = {'symbol': symbol}\n",
        "\n",
        "        # Iterate through desired features and retrieve them from info dictionary\n",
        "        for feature in desired_features:\n",
        "            if feature in info:\n",
        "                selected_info[feature] = info[feature]\n",
        "\n",
        "        return selected_info\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing symbol {symbol}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Create an empty DataFrame to store the selected information\n",
        "selected_info_df = pd.DataFrame(columns=['symbol'] + desired_features)\n",
        "\n",
        "# Use ThreadPoolExecutor for parallel processing\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    # Submit tasks to fetch information for each symbol concurrently\n",
        "    symbol_info = executor.map(fetch_info, symbols_df['symbol'])\n",
        "\n",
        "    # Process the results\n",
        "    for info in symbol_info:\n",
        "        if info is not None:\n",
        "            selected_info_df = selected_info_df.append(info, ignore_index=True)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "selected_info_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(\"Symbol information extracted and saved to\", output_file)\n"
      ],
      "metadata": {
        "id": "YWYDxff25TWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file containing sectors and symbols\n",
        "df = pd.read_csv('/content/symbol_info.csv')\n",
        "\n",
        "# Define a function to clean a single symbol\n",
        "def clean_symbol(symbol):\n",
        "    return symbol.replace('.NS', '').replace('.BO', '')\n",
        "\n",
        "# Apply the cleaning function to each symbol in the 'Symbols' column\n",
        "df['symbol'] = df['symbol'].apply(lambda symbols: ','.join(clean_symbol(symbol) for symbol in symbols.split(',')))\n",
        "\n",
        "# Save the cleaned DataFrame to a new CSV file\n",
        "df.to_csv('cleaned_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "lbtz196hDRsb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/cleaned_data.csv')\n",
        "\n",
        "# Remove duplicate rows based on values in the first column\n",
        "df.drop_duplicates(subset=df.columns[0], keep='first', inplace=True)\n",
        "\n",
        "# Save the cleaned DataFrame to a new CSV file\n",
        "df.to_csv('cleaned_data_duplicate.csv', index=False)\n"
      ],
      "metadata": {
        "id": "mfZdSrctDW1e"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/cleaned_data_duplicate.csv')\n",
        "\n",
        "# Sort the DataFrame by 'Sector' column, and within each group, sort by other columns\n",
        "df_sorted = df.sort_values(by=['sector', 'symbol', 'shortName', 'longName', 'industry'])\n",
        "\n",
        "# Save the sorted DataFrame to a new CSV file\n",
        "df_sorted.to_csv('sorted_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "MYjjxoD4DXxV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/sorted_data.csv')\n",
        "\n",
        "# Define a regular expression pattern to match specific special symbols\n",
        "pattern = r'[()@?,&-]'  # Matches any of the specified special symbols\n",
        "\n",
        "# Remove specific special symbols from all columns\n",
        "df.replace({pattern: ''}, regex=True, inplace=True)\n",
        "\n",
        "# Save the modified DataFrame back to CSV\n",
        "df.to_csv('modified_file.csv', index=False)\n",
        "\n",
        "print(\"Specific special symbols removed and saved to modified_file.csv\")\n"
      ],
      "metadata": {
        "id": "PGhK2ImBDaog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/modified_file.csv')\n",
        "\n",
        "# Get the unique values in the 'Sector' column\n",
        "unique_sectors = df['sector'].unique()\n",
        "\n",
        "# Iterate over each unique sector\n",
        "for sector in unique_sectors:\n",
        "    # Filter the DataFrame for the current sector\n",
        "    sector_df = df[df['sector'] == sector]\n",
        "\n",
        "    # Sort the DataFrame by the first column (assuming you want to sort by the first column)\n",
        "    sector_df_sorted = sector_df.sort_values(by=sector_df.columns[0])\n",
        "\n",
        "    # Drop the 'Sector' column\n",
        "    sector_df_sorted.drop(columns=['sector'], inplace=True)\n",
        "\n",
        "    # Save the sorted DataFrame to a new CSV file\n",
        "    sector_df_sorted.to_csv(f'{sector}_wisesort.csv', index=False)\n"
      ],
      "metadata": {
        "id": "drCq6A1LDdFT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of CSV files with their names\n",
        "csv_files = [\n",
        "    (\"/content/Basic Materials_wisesort.csv\", \"Basic Materials_wisesort.csv\"),\n",
        "    (\"/content/Communication Services_wisesort.csv\", \"Communication Services_wisesort.cs\"),\n",
        "    (\"/content/Consumer Cyclical_wisesort.csv\", \"Consumer Cyclical_wisesort.csv\"),\n",
        "     (\"/content/Consumer Defensive_wisesort.csv\", \"Consumer Defensive_wisesort.csv\"),\n",
        "     (\"/content/Energy_wisesort.csv\", \"Energy_wisesort.csv\"),\n",
        "     (\"/content/Financial Services_wisesort.csv\", \"Financial Services_wisesort.csv\"),\n",
        "     (\"/content/Healthcare_wisesort.csv\", \"Healthcare_wisesort.csv\"),\n",
        "     (\"/content/Industrials_wisesort.csv\", \"Industrials_wisesort.csv\"),\n",
        "     (\"/content/Real Estate_wisesort.csv\", \"Real Estate_wisesort.csv\"),\n",
        "     (\"/content/Technology_wisesort.csv\", \"Technology_wisesort.csv\"),\n",
        "     (\"/content/Utilities_wisesort.csv\", \"Utilities_wisesort.csv\"),\n",
        "     (\"/content/nan_wisesort.csv\", \"nan_wisesort.csv\"),\n",
        "    # Add more CSV files with their names as needed\n",
        "]\n",
        "\n",
        "# Iterate over each CSV file\n",
        "for csv_file, name in csv_files:\n",
        "    # Create an empty DataFrame to store the individual words\n",
        "    df_all_words = pd.DataFrame(columns=['All Words'])\n",
        "    # Read the original CSV file\n",
        "    df = pd.read_csv(csv_file)\n",
        "    # Iterate over each cell in the original DataFrame\n",
        "    for col in df.columns:\n",
        "        for cell in df[col]:\n",
        "            # Split the text into words\n",
        "            words = str(cell).split()\n",
        "            # Add each word to the new DataFrame\n",
        "            for word in words:\n",
        "                df_all_words = df_all_words.append({'All Words': word}, ignore_index=True)\n",
        "\n",
        "    # Save the DataFrame to a new CSV file with the name of the source CSV file\n",
        "    output_file = f'all_words_{name}.csv'\n",
        "    df_all_words.to_csv(output_file, index=False)\n",
        "    print(f\"Individual words extracted and saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "daemi5AoDfQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of CSV files\n",
        "csv_files = [\n",
        "   \"/content/all_words_Basic Materials_wisesort.csv.csv\",\n",
        "    \"/content/all_words_Communication Services_wisesort.cs.csv\",\n",
        "    \"/content/all_words_Consumer Cyclical_wisesort.csv.csv\",\n",
        "    \"/content/all_words_Consumer Defensive_wisesort.csv.csv\",\n",
        "    \"/content/all_words_Energy_wisesort.csv.csv\",\n",
        "    \"/content/all_words_Financial Services_wisesort.csv.csv\",\n",
        "    \"/content/all_words_Healthcare_wisesort.csv.csv\",\n",
        "    \"/content/all_words_Industrials_wisesort.csv.csv\",\n",
        "    \"/content/all_words_Real Estate_wisesort.csv.csv\",\n",
        "    \"/content/all_words_Technology_wisesort.csv.csv\",\n",
        "    \"/content/all_words_Utilities_wisesort.csv.csv\",\n",
        "    \"/content/all_words_nan_wisesort.csv.csv\"\n",
        "    # Add more CSV files as needed\n",
        "]\n",
        "\n",
        "# Iterate over each CSV file\n",
        "for csv_file in csv_files:\n",
        "    # Set to store unique words for each file\n",
        "    unique_words = set()\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(csv_file)\n",
        "    # Iterate over each row in the DataFrame\n",
        "    for row in df.itertuples(index=False):\n",
        "        # Split the text into words and add them to the set\n",
        "        words = str(row[0]).split()  # Assuming the text is in the first column\n",
        "        unique_words.update(words)\n",
        "    # Convert the set of unique words to a DataFrame\n",
        "    df_unique_words = pd.DataFrame(unique_words, columns=['Unique Words'])\n",
        "    # Save the DataFrame to a new CSV file\n",
        "    output_file = f\"unique_words_{csv_file.split('/')[-1]}\"\n",
        "    df_unique_words.to_csv(output_file, index=False)\n",
        "    print(f\"Unique words extracted and saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "IM22GFg7Dgyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# List of CSV files containing unique words\n",
        "csv_files = [\n",
        "    \"/content/unique_words_all_words_Basic Materials_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Communication Services_wisesort.cs.csv\",\n",
        "    \"/content/unique_words_all_words_Consumer Cyclical_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Consumer Defensive_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Energy_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Financial Services_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Healthcare_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Industrials_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Real Estate_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Technology_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Utilities_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_nan_wisesort.csv.csv\"\n",
        "    # Add more CSV files as needed\n",
        "]\n",
        "\n",
        "# Read the headlines CSV file\n",
        "headlines_df = pd.read_csv(\"/content/cleaned_combined.csv\")\n",
        "\n",
        "# Iterate over each CSV file containing unique words\n",
        "for csv_file in csv_files:\n",
        "    # Read the CSV file containing unique words\n",
        "    unique_words_df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Initialize an empty DataFrame to store filtered headlines\n",
        "    filtered_headlines = pd.DataFrame(columns=headlines_df.columns)\n",
        "\n",
        "    # Get the filename (without extension) for the output CSV file\n",
        "    output_filename = os.path.splitext(os.path.basename(csv_file))[0] + \"_filtered_headlines.csv\"\n",
        "\n",
        "    # Iterate over each unique word in the CSV file\n",
        "    for word in unique_words_df[\"Unique Words\"]:\n",
        "        # Ensure that the word is a string\n",
        "        word = str(word)\n",
        "\n",
        "        # Search for the word in the headlines DataFrame\n",
        "        filtered_rows = headlines_df[headlines_df[\"Headline\"].str.contains(word, case=False)]\n",
        "\n",
        "        # Append the filtered rows to the DataFrame\n",
        "        filtered_headlines = pd.concat([filtered_headlines, filtered_rows])\n",
        "\n",
        "    # Remove duplicate rows\n",
        "    filtered_headlines.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Save the filtered headlines to a new CSV file\n",
        "    filtered_headlines.to_csv(output_filename, index=False)\n",
        "\n",
        "    print(f\"Filtered headlines for {csv_file} saved to {output_filename}\")\n"
      ],
      "metadata": {
        "id": "_qfDYw5FNh1v",
        "outputId": "9a199c83-9e5b-4f46-9c98-2a65bfc58195",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered headlines for /content/unique_words_all_words_Basic Materials_wisesort.csv.csv saved to unique_words_all_words_Basic Materials_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Communication Services_wisesort.cs.csv saved to unique_words_all_words_Communication Services_wisesort.cs_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Consumer Cyclical_wisesort.csv.csv saved to unique_words_all_words_Consumer Cyclical_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Consumer Defensive_wisesort.csv.csv saved to unique_words_all_words_Consumer Defensive_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Energy_wisesort.csv.csv saved to unique_words_all_words_Energy_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Financial Services_wisesort.csv.csv saved to unique_words_all_words_Financial Services_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Healthcare_wisesort.csv.csv saved to unique_words_all_words_Healthcare_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Industrials_wisesort.csv.csv saved to unique_words_all_words_Industrials_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Real Estate_wisesort.csv.csv saved to unique_words_all_words_Real Estate_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Technology_wisesort.csv.csv saved to unique_words_all_words_Technology_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Utilities_wisesort.csv.csv saved to unique_words_all_words_Utilities_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_nan_wisesort.csv.csv saved to unique_words_all_words_nan_wisesort.csv_filtered_headlines.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# List of CSV files containing unique words\n",
        "csv_files = [\n",
        "    \"/content/unique_words_all_words_Basic Materials_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Communication Services_wisesort.cs.csv\",\n",
        "    \"/content/unique_words_all_words_Consumer Cyclical_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Consumer Defensive_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Energy_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Financial Services_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Healthcare_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Industrials_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Real Estate_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Technology_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_Utilities_wisesort.csv.csv\",\n",
        "    \"/content/unique_words_all_words_nan_wisesort.csv.csv\"\n",
        "    # Add more CSV files as needed\n",
        "]\n",
        "\n",
        "# Read the headlines CSV file\n",
        "headlines_df = pd.read_csv(\"/content/cleaned_combined.csv\")\n",
        "\n",
        "# Words to be removed from headlines\n",
        "words_to_remove = [\"the\", \"an\", \"of\", \"for\", \"from\", \"his\", \"him\"]\n",
        "\n",
        "# Iterate over each CSV file containing unique words\n",
        "for csv_file in csv_files:\n",
        "    # Read the CSV file containing unique words\n",
        "    unique_words_df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Initialize an empty DataFrame to store filtered headlines\n",
        "    filtered_headlines = pd.DataFrame(columns=headlines_df.columns)\n",
        "\n",
        "    # Get the filename (without extension) for the output CSV file\n",
        "    output_filename = os.path.splitext(os.path.basename(csv_file))[0] + \"_filtered_headlines.csv\"\n",
        "\n",
        "    # Iterate over each unique word in the CSV file\n",
        "    for word in unique_words_df[\"Unique Words\"]:\n",
        "        # Ensure that the word is a string\n",
        "        word = str(word)\n",
        "\n",
        "        # Check if the word is not in the words_to_remove list\n",
        "        if word.lower() not in words_to_remove:\n",
        "            # Search for the word in the headlines DataFrame\n",
        "            filtered_rows = headlines_df[headlines_df[\"Headline\"].str.contains(word, case=False)]\n",
        "\n",
        "            # Append the filtered rows to the DataFrame\n",
        "            filtered_headlines = pd.concat([filtered_headlines, filtered_rows])\n",
        "\n",
        "    # Remove duplicate rows\n",
        "    filtered_headlines.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Save the filtered headlines to a new CSV file\n",
        "    filtered_headlines.to_csv(output_filename, index=False)\n",
        "\n",
        "    print(f\"Filtered headlines for {csv_file} saved to {output_filename}\")\n"
      ],
      "metadata": {
        "id": "9-jy_PCmSuVA",
        "outputId": "085e0be2-2f84-4a17-e6a5-c5fd4840c474",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered headlines for /content/unique_words_all_words_Basic Materials_wisesort.csv.csv saved to unique_words_all_words_Basic Materials_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Communication Services_wisesort.cs.csv saved to unique_words_all_words_Communication Services_wisesort.cs_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Consumer Cyclical_wisesort.csv.csv saved to unique_words_all_words_Consumer Cyclical_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Consumer Defensive_wisesort.csv.csv saved to unique_words_all_words_Consumer Defensive_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Energy_wisesort.csv.csv saved to unique_words_all_words_Energy_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Financial Services_wisesort.csv.csv saved to unique_words_all_words_Financial Services_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Healthcare_wisesort.csv.csv saved to unique_words_all_words_Healthcare_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Industrials_wisesort.csv.csv saved to unique_words_all_words_Industrials_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Real Estate_wisesort.csv.csv saved to unique_words_all_words_Real Estate_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Technology_wisesort.csv.csv saved to unique_words_all_words_Technology_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_Utilities_wisesort.csv.csv saved to unique_words_all_words_Utilities_wisesort.csv_filtered_headlines.csv\n",
            "Filtered headlines for /content/unique_words_all_words_nan_wisesort.csv.csv saved to unique_words_all_words_nan_wisesort.csv_filtered_headlines.csv\n"
          ]
        }
      ]
    }
  ]
}