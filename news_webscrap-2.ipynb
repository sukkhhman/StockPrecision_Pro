{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_headlines(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Extract the news headlines\n",
        "        headlines = soup.find_all('h2')\n",
        "        return [headline.text.strip() for headline in headlines]\n",
        "    else:\n",
        "        print(f\"Failed to fetch page: {url}\")\n",
        "        return []\n",
        "\n",
        "def save_to_csv(headlines, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Headline'])\n",
        "        for headline in headlines:\n",
        "            writer.writerow([headline])\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    # Base URL for the news page\n",
        "    base_url = 'https://www.moneycontrol.com/news/business/page-'\n",
        "    # Number of pages to scrape\n",
        "    num_pages = 30\n",
        "\n",
        "    all_headlines = []\n",
        "    # Iterate over each page and scrape headlines\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}{page_num}/\"\n",
        "        print(f\"Scraping headlines from page {page_num}...\")\n",
        "        headlines = scrape_headlines(url)\n",
        "        all_headlines.extend(headlines)\n",
        "\n",
        "    # Save the headlines to a CSV file\n",
        "    save_to_csv(all_headlines, 'moneycontrol_headlines.csv')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "b3X0gRNIl3cG",
        "outputId": "392c1a78-ba41-463b-b953-6b725d72f162",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping headlines from page 1...\n",
            "Scraping headlines from page 2...\n",
            "Scraping headlines from page 3...\n",
            "Scraping headlines from page 4...\n",
            "Scraping headlines from page 5...\n",
            "Scraping headlines from page 6...\n",
            "Scraping headlines from page 7...\n",
            "Scraping headlines from page 8...\n",
            "Scraping headlines from page 9...\n",
            "Scraping headlines from page 10...\n",
            "Scraping headlines from page 11...\n",
            "Scraping headlines from page 12...\n",
            "Scraping headlines from page 13...\n",
            "Scraping headlines from page 14...\n",
            "Scraping headlines from page 15...\n",
            "Scraping headlines from page 16...\n",
            "Scraping headlines from page 17...\n",
            "Scraping headlines from page 18...\n",
            "Scraping headlines from page 19...\n",
            "Scraping headlines from page 20...\n",
            "Scraping headlines from page 21...\n",
            "Scraping headlines from page 22...\n",
            "Scraping headlines from page 23...\n",
            "Scraping headlines from page 24...\n",
            "Scraping headlines from page 25...\n",
            "Scraping headlines from page 26...\n",
            "Scraping headlines from page 27...\n",
            "Scraping headlines from page 28...\n",
            "Scraping headlines from page 29...\n",
            "Scraping headlines from page 30...\n",
            "Data saved to moneycontrol_headlines.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_headlines(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Extract the news headlines\n",
        "        headlines = soup.find_all('h2')\n",
        "        return [headline.text.strip() for headline in headlines]\n",
        "    else:\n",
        "        print(f\"Failed to fetch page: {url}\")\n",
        "        return []\n",
        "\n",
        "def save_to_csv(headlines, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Headline'])\n",
        "        for headline in headlines:\n",
        "            writer.writerow([headline])\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    # Base URL for the news page\n",
        "    base_url = 'https://www.moneycontrol.com/news/india/page-'\n",
        "    # Number of pages to scrape\n",
        "    num_pages = 30\n",
        "\n",
        "    all_headlines = []\n",
        "    # Iterate over each page and scrape headlines\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}{page_num}/\"\n",
        "        print(f\"Scraping headlines from page {page_num}...\")\n",
        "        headlines = scrape_headlines(url)\n",
        "        all_headlines.extend(headlines)\n",
        "\n",
        "    # Save the headlines to a CSV file\n",
        "    save_to_csv(all_headlines, 'moneycontrol_headlines_india.csv')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "PJmMHtY5l4hf",
        "outputId": "8100bc71-78d9-4416-9605-6e3f7052d4f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping headlines from page 1...\n",
            "Scraping headlines from page 2...\n",
            "Scraping headlines from page 3...\n",
            "Scraping headlines from page 4...\n",
            "Scraping headlines from page 5...\n",
            "Scraping headlines from page 6...\n",
            "Scraping headlines from page 7...\n",
            "Scraping headlines from page 8...\n",
            "Scraping headlines from page 9...\n",
            "Scraping headlines from page 10...\n",
            "Scraping headlines from page 11...\n",
            "Scraping headlines from page 12...\n",
            "Scraping headlines from page 13...\n",
            "Scraping headlines from page 14...\n",
            "Scraping headlines from page 15...\n",
            "Scraping headlines from page 16...\n",
            "Scraping headlines from page 17...\n",
            "Scraping headlines from page 18...\n",
            "Scraping headlines from page 19...\n",
            "Scraping headlines from page 20...\n",
            "Scraping headlines from page 21...\n",
            "Scraping headlines from page 22...\n",
            "Scraping headlines from page 23...\n",
            "Scraping headlines from page 24...\n",
            "Scraping headlines from page 25...\n",
            "Scraping headlines from page 26...\n",
            "Scraping headlines from page 27...\n",
            "Scraping headlines from page 28...\n",
            "Scraping headlines from page 29...\n",
            "Scraping headlines from page 30...\n",
            "Data saved to moneycontrol_headlines_india.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_headlines(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Extract the news headlines\n",
        "        headlines = soup.find_all('h2')\n",
        "        return [headline.text.strip() for headline in headlines]\n",
        "    else:\n",
        "        print(f\"Failed to fetch page: {url}\")\n",
        "        return []\n",
        "\n",
        "def save_to_csv(headlines, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Headline'])\n",
        "        for headline in headlines:\n",
        "            writer.writerow([headline])\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    # Base URL for the news page\n",
        "    base_url = 'https://www.moneycontrol.com/news/business/markets/page-'\n",
        "    # Number of pages to scrape\n",
        "    num_pages = 30\n",
        "\n",
        "    all_headlines = []\n",
        "    # Iterate over each page and scrape headlines\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}{page_num}/\"\n",
        "        print(f\"Scraping headlines from page {page_num}...\")\n",
        "        headlines = scrape_headlines(url)\n",
        "        all_headlines.extend(headlines)\n",
        "\n",
        "    # Save the headlines to a CSV file\n",
        "    save_to_csv(all_headlines, 'moneycontrol_headlines_markets.csv')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "QYD-GI6WmJTD",
        "outputId": "eac079ba-8c69-454d-9966-6dc01784af69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping headlines from page 1...\n",
            "Scraping headlines from page 2...\n",
            "Scraping headlines from page 3...\n",
            "Scraping headlines from page 4...\n",
            "Scraping headlines from page 5...\n",
            "Scraping headlines from page 6...\n",
            "Scraping headlines from page 7...\n",
            "Scraping headlines from page 8...\n",
            "Scraping headlines from page 9...\n",
            "Scraping headlines from page 10...\n",
            "Scraping headlines from page 11...\n",
            "Scraping headlines from page 12...\n",
            "Scraping headlines from page 13...\n",
            "Scraping headlines from page 14...\n",
            "Scraping headlines from page 15...\n",
            "Scraping headlines from page 16...\n",
            "Scraping headlines from page 17...\n",
            "Scraping headlines from page 18...\n",
            "Scraping headlines from page 19...\n",
            "Scraping headlines from page 20...\n",
            "Scraping headlines from page 21...\n",
            "Scraping headlines from page 22...\n",
            "Scraping headlines from page 23...\n",
            "Scraping headlines from page 24...\n",
            "Scraping headlines from page 25...\n",
            "Scraping headlines from page 26...\n",
            "Scraping headlines from page 27...\n",
            "Scraping headlines from page 28...\n",
            "Scraping headlines from page 29...\n",
            "Scraping headlines from page 30...\n",
            "Data saved to moneycontrol_headlines_markets.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_headlines(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Extract the news headlines\n",
        "        headlines = soup.find_all('h2')\n",
        "        return [headline.text.strip() for headline in headlines]\n",
        "    else:\n",
        "        print(f\"Failed to fetch page: {url}\")\n",
        "        return []\n",
        "\n",
        "def save_to_csv(headlines, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Headline'])\n",
        "        for headline in headlines:\n",
        "            writer.writerow([headline])\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    # Base URL for the news page\n",
        "    base_url = 'https://www.moneycontrol.com/news/trends/page-'\n",
        "    # Number of pages to scrape\n",
        "    num_pages = 30\n",
        "\n",
        "    all_headlines = []\n",
        "    # Iterate over each page and scrape headlines\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}{page_num}/\"\n",
        "        print(f\"Scraping headlines from page {page_num}...\")\n",
        "        headlines = scrape_headlines(url)\n",
        "        all_headlines.extend(headlines)\n",
        "\n",
        "    # Save the headlines to a CSV file\n",
        "    save_to_csv(all_headlines, 'moneycontrol_headlines_trends.csv')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "uOiAsxxgmMaJ",
        "outputId": "976d6830-330c-42ea-e13b-020bf552da41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping headlines from page 1...\n",
            "Scraping headlines from page 2...\n",
            "Scraping headlines from page 3...\n",
            "Scraping headlines from page 4...\n",
            "Scraping headlines from page 5...\n",
            "Scraping headlines from page 6...\n",
            "Scraping headlines from page 7...\n",
            "Scraping headlines from page 8...\n",
            "Scraping headlines from page 9...\n",
            "Scraping headlines from page 10...\n",
            "Scraping headlines from page 11...\n",
            "Scraping headlines from page 12...\n",
            "Scraping headlines from page 13...\n",
            "Scraping headlines from page 14...\n",
            "Scraping headlines from page 15...\n",
            "Scraping headlines from page 16...\n",
            "Scraping headlines from page 17...\n",
            "Scraping headlines from page 18...\n",
            "Scraping headlines from page 19...\n",
            "Scraping headlines from page 20...\n",
            "Scraping headlines from page 21...\n",
            "Scraping headlines from page 22...\n",
            "Scraping headlines from page 23...\n",
            "Scraping headlines from page 24...\n",
            "Scraping headlines from page 25...\n",
            "Scraping headlines from page 26...\n",
            "Scraping headlines from page 27...\n",
            "Scraping headlines from page 28...\n",
            "Scraping headlines from page 29...\n",
            "Scraping headlines from page 30...\n",
            "Data saved to moneycontrol_headlines_trends.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_headlines(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Extract the news headlines\n",
        "        headlines = soup.find_all('h2')\n",
        "        return [headline.text.strip() for headline in headlines]\n",
        "    else:\n",
        "        print(f\"Failed to fetch page: {url}\")\n",
        "        return []\n",
        "\n",
        "def save_to_csv(headlines, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Headline'])\n",
        "        for headline in headlines:\n",
        "            writer.writerow([headline])\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    # Base URL for the news page\n",
        "    base_url = 'https://www.moneycontrol.com/news/news-all/page-'\n",
        "    # Number of pages to scrape\n",
        "    num_pages = 30\n",
        "\n",
        "    all_headlines = []\n",
        "    # Iterate over each page and scrape headlines\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}{page_num}/\"\n",
        "        print(f\"Scraping headlines from page {page_num}...\")\n",
        "        headlines = scrape_headlines(url)\n",
        "        all_headlines.extend(headlines)\n",
        "\n",
        "    # Save the headlines to a CSV file\n",
        "    save_to_csv(all_headlines, 'moneycontrol_headlines_latest_news.csv')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "nMh4Y3tLmYgN",
        "outputId": "a5416d2e-6822-4619-c299-ff9ebe377823",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping headlines from page 1...\n",
            "Scraping headlines from page 2...\n",
            "Scraping headlines from page 3...\n",
            "Scraping headlines from page 4...\n",
            "Scraping headlines from page 5...\n",
            "Scraping headlines from page 6...\n",
            "Scraping headlines from page 7...\n",
            "Scraping headlines from page 8...\n",
            "Scraping headlines from page 9...\n",
            "Scraping headlines from page 10...\n",
            "Scraping headlines from page 11...\n",
            "Scraping headlines from page 12...\n",
            "Scraping headlines from page 13...\n",
            "Scraping headlines from page 14...\n",
            "Scraping headlines from page 15...\n",
            "Scraping headlines from page 16...\n",
            "Scraping headlines from page 17...\n",
            "Scraping headlines from page 18...\n",
            "Scraping headlines from page 19...\n",
            "Scraping headlines from page 20...\n",
            "Scraping headlines from page 21...\n",
            "Scraping headlines from page 22...\n",
            "Scraping headlines from page 23...\n",
            "Scraping headlines from page 24...\n",
            "Scraping headlines from page 25...\n",
            "Scraping headlines from page 26...\n",
            "Scraping headlines from page 27...\n",
            "Scraping headlines from page 28...\n",
            "Scraping headlines from page 29...\n",
            "Scraping headlines from page 30...\n",
            "Data saved to moneycontrol_headlines_latest_news.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_headlines(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Extract the news headlines\n",
        "        headlines = soup.find_all('h2')\n",
        "        return [headline.text.strip() for headline in headlines]\n",
        "    else:\n",
        "        print(f\"Failed to fetch page: {url}\")\n",
        "        return []\n",
        "\n",
        "def save_to_csv(headlines, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Headline'])\n",
        "        for headline in headlines:\n",
        "            writer.writerow([headline])\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    # Base URL for the news page\n",
        "    base_url = 'https://www.moneycontrol.com/news/news-all/page-'\n",
        "    # Number of pages to scrape\n",
        "    num_pages = 30\n",
        "\n",
        "    all_headlines = []\n",
        "    # Iterate over each page and scrape headlines\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}{page_num}/\"\n",
        "        print(f\"Scraping headlines from page {page_num}...\")\n",
        "        headlines = scrape_headlines(url)\n",
        "        all_headlines.extend(headlines)\n",
        "\n",
        "    # Save the headlines to a CSV file\n",
        "    save_to_csv(all_headlines, 'moneycontrol_headlines_latest_news.csv')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "yLODJoBemVVS",
        "outputId": "84ff856c-666d-4dbf-f25d-9b450b585e34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping headlines from page 1...\n",
            "Scraping headlines from page 2...\n",
            "Scraping headlines from page 3...\n",
            "Scraping headlines from page 4...\n",
            "Scraping headlines from page 5...\n",
            "Scraping headlines from page 6...\n",
            "Scraping headlines from page 7...\n",
            "Scraping headlines from page 8...\n",
            "Scraping headlines from page 9...\n",
            "Scraping headlines from page 10...\n",
            "Scraping headlines from page 11...\n",
            "Scraping headlines from page 12...\n",
            "Scraping headlines from page 13...\n",
            "Scraping headlines from page 14...\n",
            "Scraping headlines from page 15...\n",
            "Scraping headlines from page 16...\n",
            "Scraping headlines from page 17...\n",
            "Scraping headlines from page 18...\n",
            "Scraping headlines from page 19...\n",
            "Scraping headlines from page 20...\n",
            "Scraping headlines from page 21...\n",
            "Scraping headlines from page 22...\n",
            "Scraping headlines from page 23...\n",
            "Scraping headlines from page 24...\n",
            "Scraping headlines from page 25...\n",
            "Scraping headlines from page 26...\n",
            "Scraping headlines from page 27...\n",
            "Scraping headlines from page 28...\n",
            "Scraping headlines from page 29...\n",
            "Scraping headlines from page 30...\n",
            "Data saved to moneycontrol_headlines_latest_news.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_headlines(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Extract the news headlines\n",
        "        headlines = soup.find_all('h2')\n",
        "        return [headline.text.strip() for headline in headlines]\n",
        "    else:\n",
        "        print(f\"Failed to fetch page: {url}\")\n",
        "        return []\n",
        "\n",
        "def save_to_csv(headlines, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Headline'])\n",
        "        for headline in headlines:\n",
        "            writer.writerow([headline])\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    # Base URL for the news page\n",
        "    base_url = 'https://www.moneycontrol.com/news/tags/opinion/blogs/page-'\n",
        "    # Number of pages to scrape\n",
        "    num_pages = 30\n",
        "\n",
        "    all_headlines = []\n",
        "    # Iterate over each page and scrape headlines\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}{page_num}/\"\n",
        "        print(f\"Scraping headlines from page {page_num}...\")\n",
        "        headlines = scrape_headlines(url)\n",
        "        all_headlines.extend(headlines)\n",
        "\n",
        "    # Save the headlines to a CSV file\n",
        "    save_to_csv(all_headlines, 'moneycontrol_headlines_opinion.csv')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "QSJ9fUIAmmwz",
        "outputId": "6102b4e6-b063-49f0-9b51-6fb3abb9da43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping headlines from page 1...\n",
            "Scraping headlines from page 2...\n",
            "Scraping headlines from page 3...\n",
            "Scraping headlines from page 4...\n",
            "Scraping headlines from page 5...\n",
            "Scraping headlines from page 6...\n",
            "Scraping headlines from page 7...\n",
            "Scraping headlines from page 8...\n",
            "Scraping headlines from page 9...\n",
            "Scraping headlines from page 10...\n",
            "Scraping headlines from page 11...\n",
            "Scraping headlines from page 12...\n",
            "Scraping headlines from page 13...\n",
            "Scraping headlines from page 14...\n",
            "Scraping headlines from page 15...\n",
            "Scraping headlines from page 16...\n",
            "Scraping headlines from page 17...\n",
            "Scraping headlines from page 18...\n",
            "Scraping headlines from page 19...\n",
            "Scraping headlines from page 20...\n",
            "Scraping headlines from page 21...\n",
            "Scraping headlines from page 22...\n",
            "Scraping headlines from page 23...\n",
            "Scraping headlines from page 24...\n",
            "Scraping headlines from page 25...\n",
            "Scraping headlines from page 26...\n",
            "Scraping headlines from page 27...\n",
            "Scraping headlines from page 28...\n",
            "Scraping headlines from page 29...\n",
            "Scraping headlines from page 30...\n",
            "Data saved to moneycontrol_headlines_opinion.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_headlines(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Extract the news headlines\n",
        "        headlines = soup.find_all('h2')\n",
        "        return [headline.text.strip() for headline in headlines]\n",
        "    else:\n",
        "        print(f\"Failed to fetch page: {url}\")\n",
        "        return []\n",
        "\n",
        "def save_to_csv(headlines, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Headline'])\n",
        "        for headline in headlines:\n",
        "            writer.writerow([headline])\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    # Base URL for the news page\n",
        "    base_url = 'https://www.moneycontrol.com/news/technology/page-'\n",
        "    # Number of pages to scrape\n",
        "    num_pages = 30\n",
        "\n",
        "    all_headlines = []\n",
        "    # Iterate over each page and scrape headlines\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}{page_num}/\"\n",
        "        print(f\"Scraping headlines from page {page_num}...\")\n",
        "        headlines = scrape_headlines(url)\n",
        "        all_headlines.extend(headlines)\n",
        "\n",
        "    # Save the headlines to a CSV file\n",
        "    save_to_csv(all_headlines, 'moneycontrol_headlines_technology.csv')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "4eAsoTARn3gW",
        "outputId": "58b83df1-e268-418f-db74-379e76bcde19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping headlines from page 1...\n",
            "Scraping headlines from page 2...\n",
            "Scraping headlines from page 3...\n",
            "Scraping headlines from page 4...\n",
            "Scraping headlines from page 5...\n",
            "Scraping headlines from page 6...\n",
            "Scraping headlines from page 7...\n",
            "Scraping headlines from page 8...\n",
            "Scraping headlines from page 9...\n",
            "Scraping headlines from page 10...\n",
            "Scraping headlines from page 11...\n",
            "Scraping headlines from page 12...\n",
            "Scraping headlines from page 13...\n",
            "Scraping headlines from page 14...\n",
            "Scraping headlines from page 15...\n",
            "Scraping headlines from page 16...\n",
            "Scraping headlines from page 17...\n",
            "Scraping headlines from page 18...\n",
            "Scraping headlines from page 19...\n",
            "Scraping headlines from page 20...\n",
            "Scraping headlines from page 21...\n",
            "Scraping headlines from page 22...\n",
            "Scraping headlines from page 23...\n",
            "Scraping headlines from page 24...\n",
            "Scraping headlines from page 25...\n",
            "Scraping headlines from page 26...\n",
            "Scraping headlines from page 27...\n",
            "Scraping headlines from page 28...\n",
            "Scraping headlines from page 29...\n",
            "Scraping headlines from page 30...\n",
            "Data saved to moneycontrol_headlines_technology.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_headlines(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Extract the news headlines\n",
        "        headlines = soup.find_all('h2')\n",
        "        return [headline.text.strip() for headline in headlines]\n",
        "    else:\n",
        "        print(f\"Failed to fetch page: {url}\")\n",
        "        return []\n",
        "\n",
        "def save_to_csv(headlines, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Headline'])\n",
        "        for headline in headlines:\n",
        "            writer.writerow([headline])\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    # Base URL for the news page\n",
        "    base_url = 'https://www.moneycontrol.com/news/auto/page-'\n",
        "    # Number of pages to scrape\n",
        "    num_pages = 30\n",
        "\n",
        "    all_headlines = []\n",
        "    # Iterate over each page and scrape headlines\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}{page_num}/\"\n",
        "        print(f\"Scraping headlines from page {page_num}...\")\n",
        "        headlines = scrape_headlines(url)\n",
        "        all_headlines.extend(headlines)\n",
        "\n",
        "    # Save the headlines to a CSV file\n",
        "    save_to_csv(all_headlines, 'moneycontrol_headlines_auto.csv')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "XXNBHzwZn7aw",
        "outputId": "e411efba-44f5-4103-99df-cd02f1551436",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping headlines from page 1...\n",
            "Scraping headlines from page 2...\n",
            "Scraping headlines from page 3...\n",
            "Scraping headlines from page 4...\n",
            "Scraping headlines from page 5...\n",
            "Scraping headlines from page 6...\n",
            "Scraping headlines from page 7...\n",
            "Scraping headlines from page 8...\n",
            "Scraping headlines from page 9...\n",
            "Scraping headlines from page 10...\n",
            "Scraping headlines from page 11...\n",
            "Scraping headlines from page 12...\n",
            "Scraping headlines from page 13...\n",
            "Scraping headlines from page 14...\n",
            "Scraping headlines from page 15...\n",
            "Scraping headlines from page 16...\n",
            "Scraping headlines from page 17...\n",
            "Scraping headlines from page 18...\n",
            "Scraping headlines from page 19...\n",
            "Scraping headlines from page 20...\n",
            "Scraping headlines from page 21...\n",
            "Scraping headlines from page 22...\n",
            "Scraping headlines from page 23...\n",
            "Scraping headlines from page 24...\n",
            "Scraping headlines from page 25...\n",
            "Scraping headlines from page 26...\n",
            "Scraping headlines from page 27...\n",
            "Scraping headlines from page 28...\n",
            "Scraping headlines from page 29...\n",
            "Scraping headlines from page 30...\n",
            "Data saved to moneycontrol_headlines_auto.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_headlines(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Extract the news headlines\n",
        "        headlines = soup.find_all('h2')\n",
        "        return [headline.text.strip() for headline in headlines]\n",
        "    else:\n",
        "        print(f\"Failed to fetch page: {url}\")\n",
        "        return []\n",
        "\n",
        "def save_to_csv(headlines, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Headline'])\n",
        "        for headline in headlines:\n",
        "            writer.writerow([headline])\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    # Base URL for the news page\n",
        "    base_url = 'https://www.moneycontrol.com/news/fintech/page-'\n",
        "    # Number of pages to scrape\n",
        "    num_pages = 30\n",
        "\n",
        "    all_headlines = []\n",
        "    # Iterate over each page and scrape headlines\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}{page_num}/\"\n",
        "        print(f\"Scraping headlines from page {page_num}...\")\n",
        "        headlines = scrape_headlines(url)\n",
        "        all_headlines.extend(headlines)\n",
        "\n",
        "    # Save the headlines to a CSV file\n",
        "    save_to_csv(all_headlines, 'moneycontrol_headlines_fintech.csv')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "9xlpPT3xoBwy",
        "outputId": "63e392b3-6b82-45a2-8eca-6de2da6d78b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping headlines from page 1...\n",
            "Scraping headlines from page 2...\n",
            "Scraping headlines from page 3...\n",
            "Scraping headlines from page 4...\n",
            "Scraping headlines from page 5...\n",
            "Scraping headlines from page 6...\n",
            "Scraping headlines from page 7...\n",
            "Scraping headlines from page 8...\n",
            "Scraping headlines from page 9...\n",
            "Scraping headlines from page 10...\n",
            "Scraping headlines from page 11...\n",
            "Scraping headlines from page 12...\n",
            "Scraping headlines from page 13...\n",
            "Scraping headlines from page 14...\n",
            "Scraping headlines from page 15...\n",
            "Scraping headlines from page 16...\n",
            "Scraping headlines from page 17...\n",
            "Scraping headlines from page 18...\n",
            "Scraping headlines from page 19...\n",
            "Scraping headlines from page 20...\n",
            "Scraping headlines from page 21...\n",
            "Scraping headlines from page 22...\n",
            "Scraping headlines from page 23...\n",
            "Scraping headlines from page 24...\n",
            "Scraping headlines from page 25...\n",
            "Scraping headlines from page 26...\n",
            "Scraping headlines from page 27...\n",
            "Scraping headlines from page 28...\n",
            "Scraping headlines from page 29...\n",
            "Scraping headlines from page 30...\n",
            "Data saved to moneycontrol_headlines_fintech.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_headlines(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Extract the news headlines\n",
        "        headlines = soup.find_all('h2')\n",
        "        return [headline.text.strip() for headline in headlines]\n",
        "    else:\n",
        "        print(f\"Failed to fetch page: {url}\")\n",
        "        return []\n",
        "\n",
        "def save_to_csv(headlines, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Headline'])\n",
        "        for headline in headlines:\n",
        "            writer.writerow([headline])\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    # Base URL for the news page\n",
        "    base_url = 'https://www.moneycontrol.com/news/photos/page-'\n",
        "    # Number of pages to scrape\n",
        "    num_pages = 30\n",
        "\n",
        "    all_headlines = []\n",
        "    # Iterate over each page and scrape headlines\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}{page_num}/\"\n",
        "        print(f\"Scraping headlines from page {page_num}...\")\n",
        "        headlines = scrape_headlines(url)\n",
        "        all_headlines.extend(headlines)\n",
        "\n",
        "    # Save the headlines to a CSV file\n",
        "    save_to_csv(all_headlines, 'moneycontrol_headlines_photos.csv')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "PprkaGI3oIa-",
        "outputId": "cf9930b0-8bff-4296-e590-aac5693e2f8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping headlines from page 1...\n",
            "Scraping headlines from page 2...\n",
            "Scraping headlines from page 3...\n",
            "Scraping headlines from page 4...\n",
            "Scraping headlines from page 5...\n",
            "Scraping headlines from page 6...\n",
            "Scraping headlines from page 7...\n",
            "Scraping headlines from page 8...\n",
            "Scraping headlines from page 9...\n",
            "Scraping headlines from page 10...\n",
            "Scraping headlines from page 11...\n",
            "Scraping headlines from page 12...\n",
            "Scraping headlines from page 13...\n",
            "Scraping headlines from page 14...\n",
            "Scraping headlines from page 15...\n",
            "Scraping headlines from page 16...\n",
            "Scraping headlines from page 17...\n",
            "Scraping headlines from page 18...\n",
            "Scraping headlines from page 19...\n",
            "Scraping headlines from page 20...\n",
            "Scraping headlines from page 21...\n",
            "Scraping headlines from page 22...\n",
            "Scraping headlines from page 23...\n",
            "Scraping headlines from page 24...\n",
            "Scraping headlines from page 25...\n",
            "Scraping headlines from page 26...\n",
            "Scraping headlines from page 27...\n",
            "Scraping headlines from page 28...\n",
            "Scraping headlines from page 29...\n",
            "Scraping headlines from page 30...\n",
            "Data saved to moneycontrol_headlines_photos.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_headlines(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Extract the news headlines\n",
        "        headlines = soup.find_all('h2')\n",
        "        return [headline.text.strip() for headline in headlines]\n",
        "    else:\n",
        "        print(f\"Failed to fetch page: {url}\")\n",
        "        return []\n",
        "\n",
        "def save_to_csv(headlines, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Headline'])\n",
        "        for headline in headlines:\n",
        "            writer.writerow([headline])\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    # Base URL for the news page\n",
        "    base_url = 'https://www.moneycontrol.com/news/infographic/page-'\n",
        "    # Number of pages to scrape\n",
        "    num_pages = 30\n",
        "\n",
        "    all_headlines = []\n",
        "    # Iterate over each page and scrape headlines\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}{page_num}/\"\n",
        "        print(f\"Scraping headlines from page {page_num}...\")\n",
        "        headlines = scrape_headlines(url)\n",
        "        all_headlines.extend(headlines)\n",
        "\n",
        "    # Save the headlines to a CSV file\n",
        "    save_to_csv(all_headlines, 'moneycontrol_headlines_infographic.csv')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "QClVLnKjoSwN",
        "outputId": "a0d08e37-a0cb-4897-dbf9-785427c1d63b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping headlines from page 1...\n",
            "Scraping headlines from page 2...\n",
            "Scraping headlines from page 3...\n",
            "Scraping headlines from page 4...\n",
            "Scraping headlines from page 5...\n",
            "Scraping headlines from page 6...\n",
            "Scraping headlines from page 7...\n",
            "Scraping headlines from page 8...\n",
            "Scraping headlines from page 9...\n",
            "Scraping headlines from page 10...\n",
            "Scraping headlines from page 11...\n",
            "Scraping headlines from page 12...\n",
            "Scraping headlines from page 13...\n",
            "Scraping headlines from page 14...\n",
            "Scraping headlines from page 15...\n",
            "Scraping headlines from page 16...\n",
            "Scraping headlines from page 17...\n",
            "Scraping headlines from page 18...\n",
            "Scraping headlines from page 19...\n",
            "Scraping headlines from page 20...\n",
            "Scraping headlines from page 21...\n",
            "Scraping headlines from page 22...\n",
            "Scraping headlines from page 23...\n",
            "Scraping headlines from page 24...\n",
            "Scraping headlines from page 25...\n",
            "Scraping headlines from page 26...\n",
            "Scraping headlines from page 27...\n",
            "Scraping headlines from page 28...\n",
            "Scraping headlines from page 29...\n",
            "Scraping headlines from page 30...\n",
            "Data saved to moneycontrol_headlines_infographic.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Requesting the website\n",
        "URL = 'https://economictimes.indiatimes.com/markets/stocks?from=mdr'\n",
        "response = requests.get(URL)\n",
        "print(\"The response code is:\", response)\n",
        "\n",
        "# Parse the HTML Document\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract the news headlines from <li> tags\n",
        "headlines = soup.find_all('li')\n",
        "\n",
        "# Save headlines to a CSV file\n",
        "with open('economictimes_headlines.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Headline'])\n",
        "    for headline in headlines:\n",
        "        writer.writerow([headline.text.strip()])\n",
        "\n",
        "print(\"Headlines saved to economictimes_headlines.csv\")\n"
      ],
      "metadata": {
        "id": "Fv9i7LJMoYy-",
        "outputId": "c824af8b-d6d1-4409-9f27-a3caedc08e6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The response code is: <Response [200]>\n",
            "Headlines saved to economictimes_headlines.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Requesting the website\n",
        "URL = 'https://economictimes.indiatimes.com/markets/stocks?from=mdr'\n",
        "response = requests.get(URL)\n",
        "print(\"The response code is:\", response)\n",
        "\n",
        "# Parse the HTML Document\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract the news headlines from <a> tags\n",
        "headlines = soup.find_all('a')\n",
        "\n",
        "# Save headlines to a CSV file\n",
        "with open('economictimes_headlines2.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Headline'])\n",
        "    for headline in headlines:\n",
        "        writer.writerow([headline.text.strip()])\n",
        "\n",
        "print(\"Headlines saved to economictimes_headlines2.csv\")\n"
      ],
      "metadata": {
        "id": "PNKcgd02ofL0",
        "outputId": "a49e4f13-eab2-46ee-e95e-422a1fc23bf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The response code is: <Response [200]>\n",
            "Headlines saved to economictimes_headlines2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Requesting the website\n",
        "URL = 'https://www.cnbctv18.com/market-live/'\n",
        "response = requests.get(URL)\n",
        "print(\"The response code is:\", response)\n",
        "\n",
        "# Parse the HTML Document\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract the news headlines from <a> tags\n",
        "headlines = soup.find_all('p')\n",
        "\n",
        "# Save headlines to a CSV file\n",
        "with open('cnbc_headlines.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Headline'])\n",
        "    for headline in headlines:\n",
        "        writer.writerow([headline.text.strip()])\n",
        "\n",
        "print(\"Headlines saved to cnbc_headlines.csv\")\n"
      ],
      "metadata": {
        "id": "arToH8iooh27",
        "outputId": "fdd7fc41-3295-4ff7-df85-f4ec1a575d83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The response code is: <Response [200]>\n",
            "Headlines saved to cnbc_headlines.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Requesting the website\n",
        "URL = 'https://www.cnbctv18.com/market-live/'\n",
        "response = requests.get(URL)\n",
        "print(\"The response code is:\", response)\n",
        "\n",
        "# Parse the HTML Document\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract the news headlines from <a> tags\n",
        "headlines = soup.find_all('h2')\n",
        "\n",
        "# Save headlines to a CSV file\n",
        "with open('cnbc_headlines2.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Headline'])\n",
        "    for headline in headlines:\n",
        "        writer.writerow([headline.text.strip()])\n",
        "\n",
        "print(\"Headlines saved to cnbc_headlines2.csv\")\n"
      ],
      "metadata": {
        "id": "hBz78Y_cokGr",
        "outputId": "e1dd8148-393c-4395-c77c-c317d03553fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The response code is: <Response [200]>\n",
            "Headlines saved to cnbc_headlines2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Requesting the website\n",
        "URL = 'https://www.cnbctv18.com/economy/'\n",
        "response = requests.get(URL)\n",
        "print(\"The response code is:\", response)\n",
        "\n",
        "# Parse the HTML Document\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract the news headlines from <a> tags\n",
        "headlines = soup.find_all('h3')\n",
        "\n",
        "# Save headlines to a CSV file\n",
        "with open('cnbc_headlines3.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Headline'])\n",
        "    for headline in headlines:\n",
        "        writer.writerow([headline.text.strip()])\n",
        "\n",
        "print(\"Headlines saved to cnbc_headlines3.csv\")\n"
      ],
      "metadata": {
        "id": "fPYdi1P-omZC",
        "outputId": "3733724d-4d32-42c8-ccc4-0dca3f9e96d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The response code is: <Response [200]>\n",
            "Headlines saved to cnbc_headlines3.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Requesting the website\n",
        "URL = 'https://www.cnbctv18.com/economy/'\n",
        "response = requests.get(URL)\n",
        "print(\"The response code is:\", response)\n",
        "\n",
        "# Parse the HTML Document\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract the news headlines from <a> tags\n",
        "headlines = soup.find_all('h2')\n",
        "\n",
        "# Save headlines to a CSV file\n",
        "with open('cnbc_headlines4.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Headline'])\n",
        "    for headline in headlines:\n",
        "        writer.writerow([headline.text.strip()])\n",
        "\n",
        "print(\"Headlines saved to cnbc_headlines4.csv\")\n"
      ],
      "metadata": {
        "id": "wOaeGJW2oofo",
        "outputId": "b4f802b9-6692-4686-a699-30305c6cfedb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The response code is: <Response [200]>\n",
            "Headlines saved to cnbc_headlines4.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Requesting the website\n",
        "URL = 'https://www.cnbctv18.com/economy/'\n",
        "response = requests.get(URL)\n",
        "print(\"The response code is:\", response)\n",
        "\n",
        "# Parse the HTML Document\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract the news headlines from <a> tags\n",
        "headlines = soup.find_all('a')\n",
        "\n",
        "# Save headlines to a CSV file\n",
        "with open('cnbc_headlines5.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Headline'])\n",
        "    for headline in headlines:\n",
        "        writer.writerow([headline.text.strip()])\n",
        "\n",
        "print(\"Headlines saved to cnbc_headlines5.csv\")\n"
      ],
      "metadata": {
        "id": "C6Y6BD7Aoqru",
        "outputId": "794a2076-527b-456b-bcfc-28149d0fc237",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The response code is: <Response [200]>\n",
            "Headlines saved to cnbc_headlines5.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Requesting the website\n",
        "URL = 'https://www.cnbctv18.com/auto/'\n",
        "response = requests.get(URL)\n",
        "print(\"The response code is:\", response)\n",
        "\n",
        "# Parse the HTML Document\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract the news headlines from <a> tags\n",
        "headlines = soup.find_all('h2')\n",
        "\n",
        "# Save headlines to a CSV file\n",
        "with open('cnbc_headlines_auto.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Headline'])\n",
        "    for headline in headlines:\n",
        "        writer.writerow([headline.text.strip()])\n",
        "\n",
        "print(\"Headlines saved to cnbc_headlines_auto.csv\")\n"
      ],
      "metadata": {
        "id": "4SoZQLGNosm_",
        "outputId": "2d72332d-ccbc-4bfb-9d5b-6eb85b646258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The response code is: <Response [200]>\n",
            "Headlines saved to cnbc_headlines_auto.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Requesting the website\n",
        "URL = 'https://www.cnbctv18.com/auto/'\n",
        "response = requests.get(URL)\n",
        "print(\"The response code is:\", response)\n",
        "\n",
        "# Parse the HTML Document\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract the news headlines from <a> tags\n",
        "headlines = soup.find_all('a')\n",
        "\n",
        "# Save headlines to a CSV file\n",
        "with open('cnbc_headlines_auto1.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Headline'])\n",
        "    for headline in headlines:\n",
        "        writer.writerow([headline.text.strip()])\n",
        "\n",
        "print(\"Headlines saved to cnbc_headlines_auto1.csv\")\n"
      ],
      "metadata": {
        "id": "D9i1Dv8HowER",
        "outputId": "76c7bf27-6b6b-4559-8570-084fceef0d4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The response code is: <Response [200]>\n",
            "Headlines saved to cnbc_headlines_auto1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Requesting the website\n",
        "URL = 'https://www.cnbctv18.com/business/companies/'\n",
        "response = requests.get(URL)\n",
        "print(\"The response code is:\", response)\n",
        "\n",
        "# Parse the HTML Document\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract the news headlines from <a> tags\n",
        "headlines = soup.find_all('h2')\n",
        "\n",
        "# Save headlines to a CSV file\n",
        "with open('cnbc_headlines_company.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Headline'])\n",
        "    for headline in headlines:\n",
        "        writer.writerow([headline.text.strip()])\n",
        "\n",
        "print(\"Headlines saved to cnbc_headlines_company.csv\")\n"
      ],
      "metadata": {
        "id": "wb49HJt9o0FT",
        "outputId": "1c66c897-a90c-43c2-b070-62c8bdc91a16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The response code is: <Response [200]>\n",
            "Headlines saved to cnbc_headlines_company.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Requesting the website\n",
        "URL = 'https://www.cnbctv18.com/business/companies/'\n",
        "response = requests.get(URL)\n",
        "print(\"The response code is:\", response)\n",
        "\n",
        "# Parse the HTML Document\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract the news headlines from <a> tags\n",
        "headlines = soup.find_all('a')\n",
        "\n",
        "# Save headlines to a CSV file\n",
        "with open('cnbc_headlines_company2.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Headline'])\n",
        "    for headline in headlines:\n",
        "        writer.writerow([headline.text.strip()])\n",
        "\n",
        "print(\"Headlines saved to cnbc_headlines_company2.csv\")\n"
      ],
      "metadata": {
        "id": "HnLkX-OEo1_c",
        "outputId": "04ef3090-8669-48a1-fd60-9c6d784dfd27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The response code is: <Response [200]>\n",
            "Headlines saved to cnbc_headlines_company2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Requesting the website\n",
        "URL = 'https://www.cnbctv18.com/business/companies/'\n",
        "response = requests.get(URL)\n",
        "print(\"The response code is:\", response)\n",
        "\n",
        "# Parse the HTML Document\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract the news headlines from <a> tags\n",
        "headlines = soup.find_all('div')\n",
        "\n",
        "# Save headlines to a CSV file\n",
        "with open('cnbc_headlines_company3.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Headline'])\n",
        "    for headline in headlines:\n",
        "        writer.writerow([headline.text.strip()])\n",
        "\n",
        "print(\"Headlines saved to cnbc_headlines_company3.csv\")\n"
      ],
      "metadata": {
        "id": "-TGFlzEzo3sI",
        "outputId": "aaa76356-2b23-43aa-80ae-270849050759",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The response code is: <Response [200]>\n",
            "Headlines saved to cnbc_headlines_company3.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_headlines(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Extract the news headlines\n",
        "        headlines = soup.find_all('h2')\n",
        "        return [headline.text.strip() for headline in headlines]\n",
        "    else:\n",
        "        print(f\"Failed to fetch page: {url}\")\n",
        "        return []\n",
        "\n",
        "def save_to_csv(headlines, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Headline'])\n",
        "        for headline in headlines:\n",
        "            writer.writerow([headline])\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    # Base URL for the news page\n",
        "    base_url = 'https://www.moneycontrol.com/news/world/page-'\n",
        "    # Number of pages to scrape\n",
        "    num_pages = 30\n",
        "\n",
        "    all_headlines = []\n",
        "    # Iterate over each page and scrape headlines\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        url = f\"{base_url}{page_num}/\"\n",
        "        print(f\"Scraping headlines from page {page_num}...\")\n",
        "        headlines = scrape_headlines(url)\n",
        "        all_headlines.extend(headlines)\n",
        "\n",
        "    # Save the headlines to a CSV file\n",
        "    save_to_csv(all_headlines, 'moneycontrol_headlines_world.csv')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Ka7F0lc2pDLk",
        "outputId": "1510f036-3c1d-4bc2-aa01-a73d03c16b50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping headlines from page 1...\n",
            "Scraping headlines from page 2...\n",
            "Scraping headlines from page 3...\n",
            "Scraping headlines from page 4...\n",
            "Scraping headlines from page 5...\n",
            "Scraping headlines from page 6...\n",
            "Scraping headlines from page 7...\n",
            "Scraping headlines from page 8...\n",
            "Scraping headlines from page 9...\n",
            "Scraping headlines from page 10...\n",
            "Scraping headlines from page 11...\n",
            "Scraping headlines from page 12...\n",
            "Scraping headlines from page 13...\n",
            "Scraping headlines from page 14...\n",
            "Scraping headlines from page 15...\n",
            "Scraping headlines from page 16...\n",
            "Scraping headlines from page 17...\n",
            "Scraping headlines from page 18...\n",
            "Scraping headlines from page 19...\n",
            "Scraping headlines from page 20...\n",
            "Scraping headlines from page 21...\n",
            "Scraping headlines from page 22...\n",
            "Scraping headlines from page 23...\n",
            "Scraping headlines from page 24...\n",
            "Scraping headlines from page 25...\n",
            "Scraping headlines from page 26...\n",
            "Scraping headlines from page 27...\n",
            "Scraping headlines from page 28...\n",
            "Scraping headlines from page 29...\n",
            "Scraping headlines from page 30...\n",
            "Data saved to moneycontrol_headlines_world.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of file paths for the CSV files\n",
        "file_paths = [\n",
        "    '/content/moneycontrol_headlines.csv',\n",
        "    '/content/moneycontrol_headlines_auto.csv',\n",
        "    '/content/moneycontrol_headlines_fintech.csv',\n",
        "    '/content/moneycontrol_headlines_india.csv',\n",
        "    '/content/moneycontrol_headlines_infographic.csv',\n",
        "    '/content/moneycontrol_headlines_latest_news.csv',\n",
        "    '/content/moneycontrol_headlines_markets.csv',\n",
        "    '/content/moneycontrol_headlines_opinion.csv',\n",
        "    '/content/moneycontrol_headlines_photos.csv',\n",
        "    '/content/moneycontrol_headlines_technology.csv',\n",
        "    '/content/moneycontrol_headlines_trends.csv',\n",
        "    '/content/moneycontrol_headlines_world.csv',\n",
        "    '/content/economictimes_headlines2.csv',\n",
        "    '/content/economictimes_headlines.csv',\n",
        "    '/content/cnbc_headlines.csv',\n",
        "    '/content/cnbc_headlines2.csv',\n",
        "    '/content/cnbc_headlines4.csv',\n",
        "    '/content/cnbc_headlines3.csv',\n",
        "    '/content/cnbc_headlines5.csv',\n",
        "    '/content/cnbc_headlines_auto.csv',\n",
        "    '/content/cnbc_headlines_auto1.csv',\n",
        "    '/content/cnbc_headlines_company.csv',\n",
        "    '/content/cnbc_headlines_company2.csv',\n",
        "    '/content/cnbc_headlines_company3.csv'\n",
        "]\n",
        "\n",
        "# List to store DataFrame objects\n",
        "dataframes = []\n",
        "\n",
        "# Iterate over each file path\n",
        "for file_path in file_paths:\n",
        "    # Read the CSV file into a DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    # Append the DataFrame to the list\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Concatenate all DataFrames in the list vertically\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Save the combined DataFrame to a new CSV file\n",
        "combined_csv_path = '/content/combined.csv'\n",
        "combined_df.to_csv(combined_csv_path, index=False)\n",
        "\n",
        "print(f\"Combined CSV file saved to: {combined_csv_path}\")\n"
      ],
      "metadata": {
        "id": "PYBo0H0no5rH",
        "outputId": "fcb1df28-ff74-40bf-bb70-16ea82a55ee9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined CSV file saved to: /content/combined.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "file_path = '/content/combined.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Drop rows with missing values in the 'headline' column\n",
        "df = df.dropna(subset=['Headline'])\n",
        "\n",
        "# Clean the data: filter out rows with headline column having less than 3 words\n",
        "df['word_count'] = df['Headline'].apply(lambda x: len(str(x).split()))\n",
        "df = df[df['word_count'] >= 4]\n",
        "\n",
        "# Drop the 'word_count' column\n",
        "df = df.drop(columns=['word_count'])\n",
        "\n",
        "# Save the cleaned DataFrame back to a CSV file\n",
        "cleaned_file_path = '/content/cleaned_combined1.csv'\n",
        "df.to_csv(cleaned_file_path, index=False)\n",
        "\n",
        "print(\"Cleaned data saved to\", cleaned_file_path)\n"
      ],
      "metadata": {
        "id": "OubKt6TQpMtm",
        "outputId": "b42251b3-38b1-4396-a815-9bc537f002a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to /content/cleaned_combined1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/cleaned_combined1.csv')\n",
        "\n",
        "# Remove rows where the \"Headline\" column contains the specified string\n",
        "df = df[~df['Headline'].str.contains('You got 30 Days Trial of')]\n",
        "df = df[~df['Headline'].str.contains('Moneycontrol Selects: Top stories this morning')]\n",
        "df = df[~df['Headline'].str.contains('Moneycontrol Daily: Your Essential 7')]\n",
        "\n",
        "# Write the modified DataFrame back to a CSV file\n",
        "df.to_csv('/content/cleaned_combined.csv', index=False)\n"
      ],
      "metadata": {
        "id": "mf3N_Y3Pp4QK"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}